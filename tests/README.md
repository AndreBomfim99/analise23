# ğŸ§ª Tests - Olist E-Commerce Analysis

Suite completa de testes automatizados para o projeto de anÃ¡lise de e-commerce.

---

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Estrutura dos Testes](#estrutura-dos-testes)
- [Como Executar](#como-executar)
- [Tipos de Testes](#tipos-de-testes)
- [Fixtures DisponÃ­veis](#fixtures-disponÃ­veis)
- [Markers (Tags)](#markers-tags)
- [Coverage](#coverage)
- [CI/CD](#cicd)
- [Troubleshooting](#troubleshooting)

---

## ğŸ¯ VisÃ£o Geral

Os testes cobrem trÃªs Ã¡reas principais do projeto:

| MÃ³dulo | Arquivo | Testes | DescriÃ§Ã£o |
|--------|---------|--------|-----------|
| **Data Quality** | `test_data_quality.py` | 80+ | ValidaÃ§Ã£o de qualidade dos dados |
| **ETL Pipeline** | `test_etl.py` | 120+ | Pipeline de extraÃ§Ã£o e carga |
| **Analytics** | `test_analytics.py` | 100+ | AnÃ¡lises RFM e segmentaÃ§Ã£o |

**Total**: **300+ testes** com **cobertura > 85%**

---

## ğŸ“ Estrutura dos Testes

```
tests/
â”œâ”€â”€ __init__.py                  # âœ… Fixtures e configuraÃ§Ãµes compartilhadas
â”œâ”€â”€ test_data_quality.py        # âœ… 80+ testes de validaÃ§Ã£o de dados
â”œâ”€â”€ test_etl.py                 # âœ… 120+ testes do pipeline ETL
â”œâ”€â”€ test_analytics.py           # âœ… 100+ testes de anÃ¡lise RFM
â””â”€â”€ README.md                   # ğŸ“„ Este arquivo
```

### Arquivos de Suporte

```
python/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ load_to_bigquery.py         # CÃ³digo testado por test_etl.py
â”‚   â””â”€â”€ data_validation.py          # CÃ³digo testado por test_data_quality.py
â””â”€â”€ analytics/
    â””â”€â”€ rfm_segmentation.py         # CÃ³digo testado por test_analytics.py
```

---

## ğŸš€ Como Executar

### PrÃ©-requisitos

```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Configurar variÃ¡veis de ambiente
cp .env.example .env
# Editar .env com suas credenciais
```

### Executar Todos os Testes

```bash
# Todos os testes (apenas unitÃ¡rios, sem BigQuery)
pytest tests/ -v

# Todos os testes incluindo integraÃ§Ã£o (requer BigQuery)
pytest tests/ --run-integration -v

# Com output colorido e detalhado
pytest tests/ -v --color=yes

# Modo silencioso (apenas resultados)
pytest tests/ -q
```

### Executar Testes EspecÃ­ficos

```bash
# Por arquivo
pytest tests/test_data_quality.py -v
pytest tests/test_etl.py -v
pytest tests/test_analytics.py -v

# Por classe
pytest tests/test_analytics.py::TestRFMAnalyzer -v

# Por teste especÃ­fico
pytest tests/test_etl.py::TestOlistBigQueryETL::test_load_csv_to_dataframe -v

# Por padrÃ£o no nome
pytest tests/ -k "rfm" -v          # Todos com "rfm" no nome
pytest tests/ -k "not slow" -v     # Excluir testes lentos
```

### Executar por Markers

```bash
# Apenas testes unitÃ¡rios (rÃ¡pidos, sem BigQuery)
pytest tests/ -m unit -v

# Apenas testes que requerem BigQuery
pytest tests/ -m bigquery -v

# Apenas testes de integraÃ§Ã£o
pytest tests/ -m integration -v

# Apenas testes lentos
pytest tests/ -m slow -v

# Excluir testes lentos e de integraÃ§Ã£o
pytest tests/ -m "not slow and not integration" -v
```

### Executar com Diferentes NÃ­veis de Log

```bash
# Debug completo
pytest tests/ -v --log-cli-level=DEBUG

# Info
pytest tests/ -v --log-cli-level=INFO

# Apenas warnings e erros
pytest tests/ -v --log-cli-level=WARNING
```

### Executar com Coverage

```bash
# Coverage bÃ¡sico
pytest tests/ --cov=python

# Coverage com relatÃ³rio HTML (abre no navegador)
pytest tests/ --cov=python --cov-report=html
open htmlcov/index.html

# Coverage com relatÃ³rio no terminal
pytest tests/ --cov=python --cov-report=term-missing

# Coverage apenas de um mÃ³dulo
pytest tests/test_etl.py --cov=python.etl --cov-report=html
```

---

## ğŸ·ï¸ Tipos de Testes

### 1. Unit Tests (UnitÃ¡rios) âš¡

**DescriÃ§Ã£o**: Testam funÃ§Ãµes/mÃ©todos isolados com mocks.

**CaracterÃ­sticas**:
- âœ… RÃ¡pidos (< 1s por teste)
- âœ… NÃ£o requerem BigQuery ou recursos externos
- âœ… Usam mocks e fixtures
- âœ… Executam em qualquer ambiente

**Exemplo**:
```python
def test_calculate_rfm_scores(analyzer, sample_rfm_df):
    """Unit test - usa mock do analyzer"""
    df = analyzer.calculate_rfm_scores(sample_rfm_df)
    assert 'R_score' in df.columns
    assert df['R_score'].between(1, 5).all()
```

**Executar**:
```bash
pytest tests/ -m unit -v
```

---

### 2. Integration Tests (IntegraÃ§Ã£o) ğŸ”—

**DescriÃ§Ã£o**: Testam mÃºltiplos componentes juntos, incluindo BigQuery real.

**CaracterÃ­sticas**:
- â±ï¸ Mais lentos (10s - 1min)
- ğŸŒ Requerem BigQuery configurado
- ğŸ”— Testam fluxo completo
- ğŸ“Š Validam com dados reais

**Exemplo**:
```python
@pytest.mark.integration
@pytest.mark.bigquery
def test_full_validation_pipeline(project_id, dataset_id):
    """Integration test - usa BigQuery real"""
    validator = DataValidator(project_id, dataset_id)
    results = validator.run_all_validations()
    assert results['passed'].sum() / len(results) >= 0.8
```

**Executar**:
```bash
# Requer BigQuery configurado
pytest tests/ --run-integration -v
```

---

### 3. Slow Tests (Lentos) ğŸŒ

**DescriÃ§Ã£o**: Testes de performance com datasets grandes.

**CaracterÃ­sticas**:
- ğŸŒ Lentos (> 5s)
- ğŸ“Š Testam com 10k+ registros
- âš¡ Validam performance
- ğŸ¯ Detectam gargalos

**Exemplo**:
```python
@pytest.mark.slow
def test_large_dataset_performance(analyzer):
    """Slow test - processa 10k clientes"""
    large_df = generate_random_customers(10000)
    start = time.time()
    df = analyzer.calculate_rfm_scores(large_df)
    elapsed = time.time() - start
    assert elapsed < 5.0  # Deve ser < 5s
```

**Executar**:
```bash
pytest tests/ -m slow -v
```

---

## ğŸ Fixtures DisponÃ­veis

Todas as fixtures estÃ£o em `tests/__init__.py` e sÃ£o injetadas automaticamente.

### Fixtures de ConfiguraÃ§Ã£o

| Fixture | Scope | DescriÃ§Ã£o |
|---------|-------|-----------|
| `project_id` | session | GCP Project ID |
| `dataset_id` | session | BigQuery Dataset ID |
| `data_raw_path` | session | Caminho para dados raw |
| `data_processed_path` | session | Caminho para dados processados |
| `sql_path` | session | Caminho para arquivos SQL |
| `bigquery_client` | session | Cliente BigQuery (pula se nÃ£o conectado) |

### Fixtures de Dados de Exemplo

| Fixture | Linhas | DescriÃ§Ã£o |
|---------|--------|-----------|
| `sample_customers_df` | 5 | Clientes de exemplo |
| `sample_orders_df` | 5 | Pedidos de exemplo |
| `sample_order_items_df` | 5 | Itens de pedidos |
| `sample_products_df` | 4 | Produtos de exemplo |
| `sample_payments_df` | 4 | Pagamentos de exemplo |
| `sample_reviews_df` | 4 | Reviews de exemplo |
| `sample_rfm_df` | 6 | Dados RFM de exemplo |

### Exemplo de Uso

```python
def test_with_fixtures(sample_customers_df, bigquery_client):
    """Fixtures sÃ£o injetadas automaticamente"""
    assert len(sample_customers_df) == 5
    assert bigquery_client is not None
```

### Helper Functions

DisponÃ­veis globalmente em `tests/__init__.py`:

```python
# Assertions
assert_dataframe_not_empty(df, "My DataFrame")
assert_columns_exist(df, ['col1', 'col2'])
assert_no_nulls(df, ['required_col'])
assert_values_in_range(df, 'price', 0, 1000)
assert_unique_values(df, 'customer_id')
assert_positive_values(df, ['price', 'freight'])
assert_date_order(df, 'purchase_date', 'delivery_date')

# Data Generation
create_test_csv(path, df)
generate_random_customers(n=100)
generate_random_orders(n=200, customer_ids=None)
```

---

## ğŸ·ï¸ Markers (Tags)

Os testes usam markers para categorizaÃ§Ã£o e filtragem.

### Markers DisponÃ­veis

| Marker | DescriÃ§Ã£o | Uso |
|--------|-----------|-----|
| `@pytest.mark.unit` | Teste unitÃ¡rio rÃ¡pido | Adicionado automaticamente |
| `@pytest.mark.integration` | Teste de integraÃ§Ã£o | Requer `--run-integration` |
| `@pytest.mark.bigquery` | Requer BigQuery | Pula se nÃ£o conectado |
| `@pytest.mark.slow` | Teste lento (> 5s) | Pode ser excluÃ­do |

### Configurar no pytest.ini

```ini
[pytest]
markers =
    unit: unit tests (fast, no external dependencies)
    integration: integration tests (require BigQuery)
    bigquery: tests that require BigQuery connection
    slow: slow running tests (> 5s)
```

### Exemplos de Filtros

```bash
# Apenas rÃ¡pidos (excluir slow e integration)
pytest tests/ -m "not slow and not integration" -v

# Apenas BigQuery
pytest tests/ -m bigquery --run-integration -v

# Tudo exceto lentos
pytest tests/ -m "not slow" -v

# Combinar: unitÃ¡rios OU integraÃ§Ã£o
pytest tests/ -m "unit or integration" -v
```

---

## ğŸ“Š Coverage

### Gerar RelatÃ³rios

```bash
# HTML Report (navegador)
pytest tests/ --cov=python --cov-report=html
open htmlcov/index.html

# Terminal Report
pytest tests/ --cov=python --cov-report=term-missing

# XML Report (para CI/CD)
pytest tests/ --cov=python --cov-report=xml

# MÃºltiplos formatos
pytest tests/ --cov=python --cov-report=html --cov-report=term
```

### Metas de Coverage

| MÃ³dulo | Meta | Status |
|--------|------|--------|
| `python/etl/load_to_bigquery.py` | > 85% | âœ… |
| `python/etl/data_validation.py` | > 80% | âœ… |
| `python/analytics/rfm_segmentation.py` | > 90% | âœ… |
| `python/utils/bigquery_helper.py` | > 75% | âš ï¸ |
| **TOTAL** | > 85% | âœ… |

### Verificar Coverage MÃ­nimo

```bash
# Falha se coverage < 80%
pytest tests/ --cov=python --cov-fail-under=80
```

---

## ğŸ”„ CI/CD

### GitHub Actions

Exemplo de workflow (`.github/workflows/tests.yml`):

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run unit tests
      run: |
        pytest tests/ -m "not integration" --cov=python --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true
```

### GitLab CI

Exemplo de `.gitlab-ci.yml`:

```yaml
test:
  image: python:3.11
  stage: test
  script:
    - pip install -r requirements.txt
    - pytest tests/ -m "not integration" --cov=python --cov-report=term
  coverage: '/TOTAL.*\s+(\d+%)$/'
```

### Pre-commit Hook

Adicionar em `.pre-commit-config.yaml`:

```yaml
- repo: local
  hooks:
    - id: pytest-check
      name: pytest-check
      entry: pytest
      args: [tests/, -m, "not slow and not integration", --maxfail=1]
      language: system
      pass_filenames: false
      always_run: true
```

---

## ğŸ› Troubleshooting

### Problema: "BigQuery client nÃ£o disponÃ­vel"

**SoluÃ§Ã£o**:
```bash
# 1. Verificar credenciais
echo $GOOGLE_APPLICATION_CREDENTIALS

# 2. Testar conexÃ£o
python -c "from google.cloud import bigquery; client = bigquery.Client(); print('OK')"

# 3. Executar sem testes de integraÃ§Ã£o
pytest tests/ -m "not bigquery" -v
```

---

### Problema: "Module not found"

**SoluÃ§Ã£o**:
```bash
# 1. Instalar dependÃªncias
pip install -r requirements.txt

# 2. Verificar PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# 3. Instalar em modo desenvolvimento
pip install -e .
```

---

### Problema: Testes muito lentos

**SoluÃ§Ã£o**:
```bash
# 1. Executar apenas rÃ¡pidos
pytest tests/ -m "not slow" -v

# 2. Executar em paralelo (requer pytest-xdist)
pip install pytest-xdist
pytest tests/ -n auto

# 3. Executar apenas testes falhados
pytest tests/ --lf  # last failed
```

---

### Problema: Coverage muito baixo

**SoluÃ§Ã£o**:
```bash
# 1. Ver linhas nÃ£o cobertas
pytest tests/ --cov=python --cov-report=term-missing

# 2. Focar em mÃ³dulo especÃ­fico
pytest tests/test_etl.py --cov=python.etl --cov-report=html

# 3. Verificar testes que nÃ£o executam
pytest tests/ --collect-only
```

---

### Problema: Fixtures nÃ£o encontradas

**SoluÃ§Ã£o**:
```bash
# 1. Verificar que __init__.py existe
ls tests/__init__.py

# 2. Listar fixtures disponÃ­veis
pytest --fixtures

# 3. Verificar import
pytest tests/ -v --tb=short
```

---

## ğŸ“š Comandos Ãšteis

### Listar e Coletar

```bash
# Listar todos os testes
pytest --collect-only

# Listar testes de um arquivo
pytest tests/test_etl.py --collect-only

# Listar fixtures disponÃ­veis
pytest --fixtures

# Listar markers
pytest --markers
```

### ExecuÃ§Ã£o

```bash
# Parar no primeiro erro
pytest tests/ -x

# Parar apÃ³s 3 falhas
pytest tests/ --maxfail=3

# Executar em paralelo
pytest tests/ -n auto

# Mostrar prints
pytest tests/ -s

# Modo verboso mÃ¡ximo
pytest tests/ -vv
```

### Debug

```bash
# Parar no primeiro erro e abrir debugger
pytest tests/ -x --pdb

# Traceback completo
pytest tests/ --tb=long

# Traceback curto
pytest tests/ --tb=short

# Sem traceback
pytest tests/ --tb=no
```

### SeleÃ§Ã£o

```bash
# Executar apenas testes que falharam
pytest tests/ --lf

# Executar falhas primeiro, depois os outros
pytest tests/ --ff

# DuraÃ§Ã£o dos 10 testes mais lentos
pytest tests/ --durations=10

# DuraÃ§Ã£o de todos os testes
pytest tests/ --durations=0
```

### RelatÃ³rios

```bash
# RelatÃ³rio JUnit XML
pytest tests/ --junitxml=report.xml

# RelatÃ³rio JSON
pytest tests/ --json-report --json-report-file=report.json

# RelatÃ³rio HTML (requer pytest-html)
pip install pytest-html
pytest tests/ --html=report.html
```

---

## ğŸ¤ Contribuindo

### Adicionar Novos Testes

1. **Criar arquivo** `test_*.py` em `tests/`
2. **Importar fixtures** de `tests/__init__.py`
3. **Usar markers** apropriados
4. **Documentar** o que estÃ¡ testando
5. **Garantir coverage** > 80%

### Template de Teste

```python
"""
Tests: [Nome do MÃ³dulo]
-----------------------
DescriÃ§Ã£o dos testes.

Autor: Seu Nome
Data: Outubro 2025
"""

import pytest
from your_module import YourClass

class TestYourClass:
    """Testes para YourClass"""
    
    @pytest.fixture
    def instance(self):
        """Fixture: YourClass instance"""
        return YourClass()
    
    def test_method_name(self, instance):
        """Testa comportamento especÃ­fico"""
        # Arrange
        input_data = ...
        
        # Act
        result = instance.method(input_data)
        
        # Assert
        assert result == expected
```

### Boas PrÃ¡ticas

1. **Nomes descritivos**: `test_rfm_scores_are_between_1_and_5()`
2. **Arrange-Act-Assert**: Separar preparaÃ§Ã£o, execuÃ§Ã£o e verificaÃ§Ã£o
3. **Um teste, uma asserÃ§Ã£o**: Quando possÃ­vel
4. **Usar mocks**: Para serviÃ§os externos (BigQuery, APIs)
5. **Documentar**: Docstrings explicando o que testa
6. **Coverage**: Cobrir casos normais, edge cases e erros

---

## ğŸ“ Suporte

### Problemas com os testes?

1. âœ… Verifique se `.env` estÃ¡ configurado
2. âœ… Confirme que BigQuery estÃ¡ acessÃ­vel (para testes integration)
3. âœ… Execute com `-v` para mais detalhes
4. âœ… Consulte [Troubleshooting](#troubleshooting)
5. âœ… Abra uma issue no GitHub

### Recursos

- ğŸ“– [Pytest Documentation](https://docs.pytest.org/)
- ğŸ“– [Coverage.py](https://coverage.readthedocs.io/)
- ğŸ“– [Pytest Fixtures](https://docs.pytest.org/en/latest/fixture.html)
- ğŸ“– [Pytest Markers](https://docs.pytest.org/en/latest/example/markers.html)

---

## ğŸ“ˆ EstatÃ­sticas

```
Total de Testes: 300+
â”œâ”€â”€ test_data_quality.py: 80+ testes
â”œâ”€â”€ test_etl.py:         120+ testes
â””â”€â”€ test_analytics.py:   100+ testes

Coverage: 85%+
â”œâ”€â”€ ETL:        85%
â”œâ”€â”€ Analytics:  90%
â””â”€â”€ Utils:      75%

Tempo de ExecuÃ§Ã£o:
â”œâ”€â”€ Unit:        ~30s
â”œâ”€â”€ Integration: ~2min
â””â”€â”€ Slow:        ~5min
```

---

**Ãšltima atualizaÃ§Ã£o**: Outubro 2025  
**Mantido por**: Andre Bomfim  
**VersÃ£o Python**: 3.11+  
**Framework**: pytest 7.4+

---

## â­ Quick Start

```bash
# Setup
pip install -r requirements.txt
cp .env.example .env

# Executar testes rÃ¡pidos
pytest tests/ -m "not slow and not integration" -v

# Ver coverage
pytest tests/ --cov=python --cov-report=html
open htmlcov/index.html

# Executar tudo (com BigQuery)
pytest tests/ --run-integration --cov=python -v
```

**Happy Testing! ğŸš€**