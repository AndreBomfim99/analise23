# üîÑ ETL Pipeline - Olist E-Commerce

Pipeline de extra√ß√£o, transforma√ß√£o e carga (ETL) dos dados do e-commerce brasileiro para Google BigQuery.

---

## üìã √çndice

1. [Vis√£o Geral](#visao-geral)
2. [Arquitetura](#arquitetura)
3. [Scripts Dispon√≠veis](#scripts)
4. [Como Executar](#execucao)
5. [Configura√ß√£o](#configuracao)
6. [Troubleshooting](#troubleshooting)

---

## üéØ Vis√£o Geral {#visao-geral}

O pipeline ETL realiza:

1. **Extract:** Leitura de CSVs do Kaggle (`data/raw/`)
2. **Transform:** Limpeza, valida√ß√£o e enriquecimento
3. **Load:** Carga no Google BigQuery

**Tecnologias:**
- Python 3.11+
- pandas
- google-cloud-bigquery
- Docker (opcional)

---

## üèóÔ∏è Arquitetura {#arquitetura}
```
python/etl/
‚îú‚îÄ‚îÄ load_to_bigquery.py      # Script principal ETL
‚îú‚îÄ‚îÄ data_validation.py        # Valida√ß√£o de qualidade
‚îú‚îÄ‚îÄ transform_data.py         # Transforma√ß√µes (opcional)
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ bigquery_schema.json  # Schema das tabelas
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ bigquery_client.py    # Cliente BigQuery
    ‚îî‚îÄ‚îÄ validators.py         # Validadores customizados
```

### **Fluxo de Dados:**
```
CSV Files (data/raw/)
    ‚Üì
[Extract] ‚Üí pd.read_csv()
    ‚Üì
[Transform] ‚Üí Limpeza + Valida√ß√£o
    ‚Üì
[Load] ‚Üí BigQuery.load_table_from_dataframe()
    ‚Üì
BigQuery Tables (olist_ecommerce dataset)
```

---

## üìú Scripts Dispon√≠veis {#scripts}

### **1. load_to_bigquery.py**

**Descri√ß√£o:** Carrega todos os CSVs para o BigQuery

**Uso:**
```bash
python python/etl/load_to_bigquery.py
```

**Par√¢metros:**
```bash
# Com argumentos opcionais
python python/etl/load_to_bigquery.py \
  --project-id your-project \
  --dataset-id olist_ecommerce \
  --data-dir data/raw/
```

**Output:**
```
‚úì Conectado ao BigQuery: your-project.olist_ecommerce
‚úì Carregando olist_orders_dataset.csv...
  ‚Üí 99,441 registros carregados
‚úì Carregando olist_customers_dataset.csv...
  ‚Üí 99,441 registros carregados
...
‚úì ETL conclu√≠do: 8 tabelas carregadas em 45s
```

---

### **2. data_validation.py**

**Descri√ß√£o:** Valida integridade e qualidade dos dados

**Uso:**
```bash
python python/etl/data_validation.py
```

**Valida√ß√µes:**
- ‚úÖ Integridade referencial (FKs)
- ‚úÖ Valores nulos cr√≠ticos
- ‚úÖ Outliers (pre√ßos, datas)
- ‚úÖ Duplicatas
- ‚úÖ Consist√™ncia temporal

**Output:**
```
üîç VALIDA√á√ÉO DE DADOS - RELAT√ìRIO
=====================================

‚úì orders: 99,441 registros
  ‚Üí 0 duplicatas
  ‚Üí 0 valores nulos cr√≠ticos
  
‚ö† order_items: 112,650 registros
  ‚Üí 15 pre√ßos = 0 (0.01%)
  ‚Üí Sugest√£o: Revisar pre√ßos zerados

‚úì customers: 99,441 registros
  ‚Üí Integridade OK

‚ùå FALHA: geolocation
  ‚Üí 5,432 zip_codes inv√°lidos (0.5%)
  
=====================================
Score de Qualidade: 92/100
```

---

### **3. transform_data.py** (Opcional)

**Descri√ß√£o:** Transforma√ß√µes avan√ßadas pr√©-carga

**Uso:**
```bash
python python/etl/transform_data.py --table orders
```

**Transforma√ß√µes:**
- Normaliza√ß√£o de datas
- C√°lculo de m√©tricas derivadas
- Enriquecimento geogr√°fico
- Tratamento de outliers

---

## ‚ñ∂Ô∏è Como Executar {#execucao}

### **M√©todo 1: Local (Python direto)**
```bash
# 1. Instalar depend√™ncias
pip install -r requirements.txt

# 2. Configurar credenciais
export GOOGLE_APPLICATION_CREDENTIALS="path/to/gcp-key.json"
export GCP_PROJECT_ID="your-project"
export GCP_DATASET_ID="olist_ecommerce"

# 3. Executar ETL
python python/etl/load_to_bigquery.py

# 4. Validar dados
python python/etl/data_validation.py
```

---

### **M√©todo 2: Docker (Recomendado)**
```bash
# 1. Build
docker-compose build etl

# 2. Executar ETL
docker-compose run etl python python/etl/load_to_bigquery.py

# 3. Validar
docker-compose run etl python python/etl/data_validation.py
```

---

### **M√©todo 3: Autom√°tico (docker-compose up)**
```bash
# Executar pipeline completo
docker-compose up etl

# Com logs detalhados
docker-compose up --verbose etl
```

---

## ‚öôÔ∏è Configura√ß√£o {#configuracao}

### **Vari√°veis de Ambiente (.env)**
```bash
# Google Cloud
GCP_PROJECT_ID=your-gcp-project
GCP_DATASET_ID=olist_ecommerce
GOOGLE_APPLICATION_CREDENTIALS=./keys/gcp-key.json

# Configura√ß√µes ETL
DATA_DIR=data/raw
BATCH_SIZE=10000
WRITE_DISPOSITION=WRITE_TRUNCATE  # ou WRITE_APPEND
```

---

### **Schema BigQuery (config/bigquery_schema.json)**
```json
{
  "orders": [
    {"name": "order_id", "type": "STRING", "mode": "REQUIRED"},
    {"name": "customer_id", "type": "STRING", "mode": "REQUIRED"},
    {"name": "order_status", "type": "STRING", "mode": "NULLABLE"},
    {"name": "order_purchase_timestamp", "type": "TIMESTAMP", "mode": "NULLABLE"},
    {"name": "order_delivered_customer_date", "type": "TIMESTAMP", "mode": "NULLABLE"}
  ],
  "customers": [
    {"name": "customer_id", "type": "STRING", "mode": "REQUIRED"},
    {"name": "customer_unique_id", "type": "STRING", "mode": "REQUIRED"},
    {"name": "customer_zip_code_prefix", "type": "STRING", "mode": "NULLABLE"},
    {"name": "customer_city", "type": "STRING", "mode": "NULLABLE"},
    {"name": "customer_state", "type": "STRING", "mode": "NULLABLE"}
  ]
}
```

---

## üîß Troubleshooting {#troubleshooting}

### **Erro: "Credentials not found"**
```bash
# Verificar credenciais
echo $GOOGLE_APPLICATION_CREDENTIALS

# Re-configurar
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/key.json"
```

---

### **Erro: "Permission denied: bigquery.tables.create"**

**Solu√ß√£o:** Garantir permiss√µes no IAM:
- `BigQuery Data Editor`
- `BigQuery Job User`
```bash
# Verificar permiss√µes
gcloud projects get-iam-policy $GCP_PROJECT_ID \
  --flatten="bindings[].members" \
  --filter="bindings.members:YOUR_EMAIL"
```

---

### **Erro: "Dataset not found"**
```bash
# Criar dataset manualmente
bq mk --dataset $GCP_PROJECT_ID:olist_ecommerce

# Ou pelo script
python -c "from google.cloud import bigquery; \
client = bigquery.Client(); \
client.create_dataset('olist_ecommerce', exists_ok=True)"
```

---

### **Erro: "CSV file not found"**
```bash
# Verificar arquivos
ls -lh data/raw/*.csv

# Re-download
kaggle datasets download olistbr/brazilian-ecommerce -p data/raw/ --unzip
```

---

### **Performance Lenta**

**Otimiza√ß√µes:**
```python
# 1. Aumentar batch size
BATCH_SIZE = 50000  # default: 10000

# 2. Usar parquet ao inv√©s de CSV
df.to_parquet('data/processed/orders.parquet')

# 3. Paralelizar cargas
from concurrent.futures import ThreadPoolExecutor
```

---

## üìä Monitoramento

### **Logs**
```bash
# Logs em tempo real
tail -f logs/etl.log

# Erros apenas
grep "ERROR" logs/etl.log
```

---

### **M√©tricas de Execu√ß√£o**
```python
# Registrar no BigQuery
INSERT INTO etl_logs (timestamp, table_name, rows_loaded, duration_seconds)
VALUES (CURRENT_TIMESTAMP(), 'orders', 99441, 12.5)
```

---

## üöÄ Pr√≥ximos Passos

Ap√≥s ETL bem-sucedido:

1. **Validar dados:** `python python/etl/data_validation.py`
2. **Executar queries SQL:** `sql/01_setup/`
3. **Rodar an√°lises:** `notebooks/`
4. **Criar dashboards:** Looker Studio

---

## üìö Refer√™ncias

- [BigQuery Python Client](https://cloud.google.com/python/docs/reference/bigquery/latest)
- [pandas Documentation](https://pandas.pydata.org/docs/)
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)

---

**‚úÖ ETL Configurado?** Prossiga para: [An√°lises SQL](../../sql/README.md)